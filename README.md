# llama-server-cpu
A (cpu-only) docker image based on ggerganov/llama.cpp

Think of it as a lightweight version of ollama, providing an easy way to run GGUF models locally. 